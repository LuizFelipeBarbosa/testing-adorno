# Model and pipeline configuration

seed: 42

data:
  train_ratio: 0.70
  val_ratio: 0.15
  test_ratio: 0.15
  max_text_length: 512
  min_text_length: 3
  stratify_by: "label_vector"

preprocessing:
  lowercase: false  # preserve case for features; model sees clean_text
  remove_urls: true
  remove_mentions: true
  normalize_unicode: true
  preserve_slang: true
  max_text_length_chars: 2000

embedding:
  model_name: "all-MiniLM-L6-v2"
  batch_size: 256
  normalize: true
  prototype_bank_size_per_label: 30
  similarity_threshold: 0.45

baseline:
  tfidf:
    max_features: 30000
    ngram_range: [1, 3]
    min_df: 2
    max_df: 0.95
    sublinear_tf: true
  logistic_regression:
    C: 1.0
    max_iter: 1000
    solver: "lbfgs"
    class_weight: "balanced"
  svm:
    C: 1.0
    kernel: "linear"
    class_weight: "balanced"

transformer:
  model_name: "distilbert-base-uncased"
  max_length: 256
  batch_size: 32
  learning_rate: 2.0e-5
  weight_decay: 0.01
  num_epochs: 5
  warmup_ratio: 0.1
  early_stopping_patience: 2
  fp16: true
  gradient_accumulation_steps: 2

ensemble:
  rule_weight: 0.25
  embedding_weight: 0.25
  model_weight: 0.50
  calibration_method: "isotonic"
  threshold_optimization_metric: "macro_f1"
  min_precision_constraint: 0.70

llm:
  model: "claude-sonnet-4-20250514"
  max_tokens: 1024
  temperature: 0.0
  max_retries: 1
  batch_size: 10
  entropy_threshold: 0.70  # route to LLM if entropy above this
  disagreement_threshold: 2  # route if >= N sources disagree

active_learning:
  pool_size: 500
  query_size: 50
  strategies:
    - "entropy"
    - "near_threshold"
    - "disagreement"
  entropy_percentile: 90
  threshold_margin: 0.10
  export_format: "csv"

logging:
  level: "INFO"
  file: "outputs/pipeline.log"
